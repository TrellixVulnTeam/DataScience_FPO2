{\rtf1\ansi\ansicpg1252\cocoartf2513
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\froman\fcharset0 Times-Bold;\f1\froman\fcharset0 Times-Roman;\f2\froman\fcharset0 Times-Italic;
\f3\ftech\fcharset77 Symbol;}
{\colortbl;\red255\green255\blue255;\red192\green0\blue0;}
{\*\expandedcolortbl;;\csgenericrgb\c75294\c0\c0;}
{\*\listtable{\list\listtemplateid1\listhybrid{\listlevel\levelnfc23\levelnfcn23\leveljc0\leveljcn0\levelfollow0\levelstartat1\levelspace360\levelindent0{\*\levelmarker \{disc\}}{\leveltext\leveltemplateid1\'01\uc0\u8226 ;}{\levelnumbers;}\fi-360\li720\lin720 }{\listname ;}\listid1}}
{\*\listoverridetable{\listoverride\listid1\listoverridecount0\ls1}}
\margl1440\margr1440\vieww10800\viewh8400\viewkind0
\deftab720
\pard\pardeftab720\li720\ri0\partightenfactor0

\f0\b\fs24 \cf0 function 
\f1\b0 GOAL-BASED-AGENT (percept) 
\f0\b returns
\f1\b0  an action                                                         \
\pard\pardeftab720\li1440\ri0\partightenfactor0

\f0\b \cf0 persistent:
\f1\b0  	state, the agent\'92s current conception of the world state                                                               \
\pard\pardeftab720\li2160\fi720\ri0\partightenfactor0
\cf2 goal, a description of the goal state\cf0 \
rules, a set of condition-action rules                                                                                           \
action, the most recent action, initially none\
\pard\pardeftab720\li720\ri0\partightenfactor0
\cf0 \
\pard\pardeftab720\li720\ri0\partightenfactor0

\f2\i \cf0 state
\f1\i0  <= UPDATE-STATE 
\f2\i (state, goal, action, percept)
\f1\i0 \

\f2\i rule
\f1\i0  <= RULE-MATCH 
\f2\i (state, goal, rules)
\f1\i0 \

\f2\i action
\f1\i0  <= 
\f2\i rule.
\f1\i0 ACTION\
\pard\pardeftab720\li720\ri0\partightenfactor0

\f0\b \cf0 return 
\f2\i\b0 action\
\pard\pardeftab720\ri0\partightenfactor0
\cf0 \
\pard\pardeftab720\ri0\partightenfactor0

\f1\i0 \cf0 Here you notice the only thing that changes is that this model uses the goal state instead of the transition model and the sensor model used in the Model-Based-Reflex-Agent. \
\
The T(n) runtime for this would be calculated as following: \
\pard\pardeftab720\li720\fi-360\ri0\partightenfactor0
\ls1\ilvl0
\f3 \cf0 \'a5	
\f1 The persistent are T(1), initialization of values from a specific percept\
\ls1\ilvl0
\f3 \'a5	
\f1 UPDATE-STATE \
\pard\pardeftab720\ri0\partightenfactor0
\cf0 \
\pard\pardeftab720\li720\ri0\partightenfactor0

\f0\b \cf0 function
\f1\b0  UTILITY-BASED-AGENT (percept)
\f0\b  returns
\f1\b0  an action                                                         \
\pard\pardeftab720\li1440\ri0\partightenfactor0

\f0\b \cf0 persistent:
\f1\b0  	state, the agent\'92s current conception of the world state                                                               \
\pard\pardeftab720\li2880\ri0\partightenfactor0
\cf2 possible states, alternative states with the max utility/happiness                                                  \
\pard\pardeftab720\li2160\fi720\ri0\partightenfactor0
\cf0 rules, a set of condition-action rules                                                                                           \
action, the most recent action, initially none\
\pard\pardeftab720\li720\ri0\partightenfactor0
\cf0 \
state <= UPDATE-STATE 
\f2\i (state, action, percept, possible states)
\f1\i0 \
rule <= RULE-MATCH 
\f2\i (state, possible states, rules)
\f1\i0 \
action <= rule.ACTION\
\pard\pardeftab720\li720\ri0\partightenfactor0

\f0\b \cf0 return
\f1\b0  action\
\pard\pardeftab720\ri0\partightenfactor0
\cf0 \
Here you notice the only thing that changes is that this model uses the goal state instead of the transition model and the sensor model used in the Model-Based-Reflex-Agent. \
\
}